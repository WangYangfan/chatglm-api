import random
import os
import numpy as np
import torch
import json
import requests

def seed_environment(seed):
    """ è®¾ç½®æ•´ä¸ªç¯å¢ƒçš„éšæœºç§å­ """
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    return

def torch_gc(device):
    if torch.cuda.is_available():
        with torch.cuda.device(device):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

def gen_response(query, history=[], max_length=None, top_p=None, temperature=None, port=None):
    """
    è°ƒç”¨æ¨¡å‹chatæ–¹æ³•ï¼Œè¾“å‡ºæ¨¡å‹å›ç­”å’Œå†å²å¯¹è¯ä¿¡æ¯

    :param history: å¦‚æœè¿ç»­è¯¢é—®ï¼Œåº”è¡¥å……historyå‚æ•°ï¼Œæ˜¯ä¸€ä¸ªå†å²å¯¹è¯åˆ—è¡¨
        if chatglm-6b or chatglm2-6b:
            åˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º2çš„åˆ—è¡¨ï¼Œç”±ä¸€é—®ä¸€ç­”ä¸¤ä¸ªå­—ç¬¦ä¸²ç»„æˆ
        if chatglm3-6b:
            å­—å…¸ç»„æˆçš„åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ åŒ…æ‹¬é”®roleå’Œcontextï¼Œç”±userå’Œassistantäº¤æ›¿ç»„æˆ

    :var full_response: full_response.json()åŒ…å« "response", "history", "status", "time", ä¾‹å¦‚ï¼š
        {
            "response":"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
            "history":[["ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],
            "status":200,
            "time":"2023-03-23 21:38:40"
        }

    :return new_response, new_history: æ¨¡å‹å›ç­”å’Œå†å²ä¿¡æ¯
    """

    url = "http://localhost:" + str(port)
    payload = json.dumps({
        "query": query, 
        "history": history, 
        "max_length": max_length, 
        "top_p": top_p, 
        "temperature": temperature
    })
    headers = {'Content-Type': 'application/json'}

    full_response = requests.request("POST", url, headers=headers, data=payload)



    new_response = full_response.json()["response"]
    new_history = full_response.json()["history"]

    return new_response, new_history


if __name__ == '__main__':

    input = {
        'query': "123 + 123 + 123 + 123 = ï¼Ÿ",
        # 'history': [['123 * 4 = ?', '123 * 4 = 496']],  # if chatglm-6b or chatglm2-6b
        # 'history': [{'role': 'user', 'content': '123 * 4 = ?'}, {'role': 'assistant', 'metadata': '', 'content': '123 * 4 = 496'}], # if chatglm3-6b
        'port': 8000,   # or 8001, 8002
    }
    request, history = gen_response(**input)
    print(request, history)
